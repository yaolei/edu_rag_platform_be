from typing import Optional, List, Dict
from fastapi import UploadFile
import asyncio
import time
import json
from pathlib import Path
from service_rag.app.embedding.embedding_data import EmbeddingData
from service_rag.app.prompt.prompt import prompt_setting
from langchain_core.documents import Document
from service_rag.app.document_operation.document_loader import DocumentLoader
from service_rag.app.text_splitter.text_split import TextSplitter
from service_rag.app.vector.vector_store import VectorStore
from service_rag.app.llm_model.contect_llm import analyze_with_image, stream_llm_response
from service_rag.app.text_splitter.advanced_text_cleaner import AdvancedTextCleaner
from service_rag.app.service.gen_util import build_simple_context, prue_image_chunks

class RagService:
    def __init__(self):
        self.embedding_type = None
        self.upload_file = None
        self.file_name = None
        self.target_file = None
        self.embeddings = None
        self.vector = None
        self.question = None
        self.file_type = None
        self.if_files = None
        self.doc_type = None
        self.mutil_files = []

        self.image_binary_data = None
        self.conversation_id = None
        self.messages = []
        self.last_doc_types = []
        self.intent_type = None

    @classmethod
    async def create(cls, upload_file: List[UploadFile]=None, embedding_type='questions', doc_type="document",
                                                             conversation_id: Optional[str] = None,
                                                             messages: Optional[List[Dict]] = None,
                                                             intent_type="chat", **kwargs):
        instance = cls()
        await instance.initialize(upload_file, embedding_type, doc_type, conversation_id=conversation_id,
                                                                                      messages=messages,
                                                                                      intent_type=intent_type,
                                                                                      **kwargs)
        return instance

    async def initialize(self, upload_file: List[UploadFile]=None, embedding_type='questions', doc_type="document",
                         conversation_id: Optional[str] = None,
                         messages: Optional[List[Dict]] = None,
                         intent_type="chat", **kwargs):
        self.embedding_type = embedding_type

        self.upload_file = upload_file
        self.doc_type = doc_type
        self.intent_type=intent_type
        self.conversation_id = conversation_id  # æ–°å¢
        self.messages = messages or []
        self.question = ""
        if self.messages:
            for msg in reversed(self.messages):
                if msg.get("role") == "user":
                    self.question = msg.get("content", "").strip()
                    break
            if self.question:
                print(f"ğŸ¯ ä»messagesä¸­æå–çš„é—®é¢˜: {self.question}")

        self.embeddings = EmbeddingData(embedding_type=embedding_type)
        self.vector = VectorStore(embedding_function=self.embeddings)

        if self.messages:
            print(f"ğŸ“š æ¥æ”¶åˆ° {len(self.messages)} æ¡å†å²æ¶ˆæ¯")

        if not upload_file:
            pass
        elif len(upload_file) == 1:
            self.if_files = False
            self.file_name = upload_file[0].filename or "unknown file"
            path_obj = Path(self.file_name)

            try:
                if upload_file[0].content_type and upload_file[0].content_type.startswith('image/'):
                    self.intent_type = 'image'
                    content = await upload_file[0].read()
                    self.image_binary_data = content
                    # é‡ç½®æ–‡ä»¶æŒ‡é’ˆï¼Œä»¥ä¾¿ DocumentLoader å¯ä»¥è¯»å–
                    await upload_file[0].seek(0)

                document_loader = DocumentLoader(upload_file[0])
                self.target_file = await document_loader.load()
                document_loader.cleanup_temp_resources()
                if self.target_file and self.target_file[0].page_content == '':
                    self.target_file = None
                else:
                    if 'document_loader' in locals():
                        document_loader.cleanup_temp_resources()
                    self.file_type = document_loader._detect_document_type()
                    return False
            except Exception as e:
                print(f"âŒ embedding module error: {str(e)}")
                raise e
        else:
            self.if_files = True
            for f in upload_file:
                document_loader_muti_file = DocumentLoader(f)
                self.mutil_files.append(await document_loader_muti_file.load())
                document_loader_muti_file.cleanup_temp_resources()
            print(f"ğŸ¯ {self.mutil_files} ğŸ¯")

    async def llava_get_content(self, prompt_sentence, image_bytes):
        """è·å–LLaVAåˆ†æç»“æœ"""
        prompt_sentence = prompt_sentence.strip()
        print(f"ğŸŒ› å‘é€ç»™LLaVAçš„æç¤ºè¯: {prompt_sentence}")
        print(f"ğŸŒ› å‘é€ç»™LLaVAçš„æç¤ºè¯é•¿åº¦: {len(prompt_sentence)}")

        final_answer = await analyze_with_image(
            image_bytes=image_bytes,
            question=prompt_sentence,
        )

        print(f"ğŸŒŸ åˆ†æçš„ç»“æœ: {final_answer}")

        if isinstance(final_answer, dict) and 'content' in final_answer:
            result_content = final_answer['content'].strip()
        else:
            result_content = str(final_answer).strip()

        print(f"ğŸŒ› LLaVAè¿”å›ç»“æœé•¿åº¦: {len(result_content)}")
        return result_content

    async def analyse_image_information(self):
        """
        åˆ†æå›¾ç‰‡ä¿¡æ¯ - ç»Ÿä¸€ä½¿ç”¨messageæ•°ç»„æ¨¡å¼
        """
        try:
            image_byte_content = self.image_binary_data
            print(f"âœ… ä½¿ç”¨ç¼“å­˜çš„å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®: {len(image_byte_content)} å­—èŠ‚")
            user_question = ""
            if self.messages:
                for msg in reversed(self.messages):
                    if msg.get("role") == "user":
                        user_question = msg.get("content", "").strip()
                        break
            # çº¯å›¾ç‰‡ï¼Œæ›´å€¾å‘äººç‰©é£æ™¯å›¾
            is_pure_image = not self.target_file
            if is_pure_image:
                print(f"ğŸ¯ è¿›å…¥çº¯å›¾ç‰‡åˆ†æåˆ†æ”¯ï¼Œç”¨æˆ·çš„é—®é¢˜: {user_question}")
                tmp_question = user_question if user_question else "è¯·æè¿°ä¸‹è¿™å¼ å›¾ç‰‡çš„å†…å®¹"
                prue_image_prompt = prompt_setting.pure_image_qa_template.format(question=tmp_question)
                result_content = await self.llava_get_content(
                    prue_image_prompt,
                    image_byte_content,
                )
                # å°†ç»“æœæµå¼è¿”å›
                chunks = prue_image_chunks(result_content)
                for chunk in chunks:
                    if not chunk.strip():
                        continue
                    data = {"choices": [{"delta": {"content": chunk + " "}}]}
                    yield f"data: {json.dumps(data)}\n\n"

                yield "data: [DONE]\n\n"
                return
            else:
                # ========== å›¾æ–‡å¤„ç†æ¨¡å¼ ==========
                print(f"ğŸ¦ å¼€å§‹åˆ†æå›¾æ–‡ä¿¡æ¯")

                # æå–OCRæ–‡æœ¬
                ocr_text = self.target_file[0].page_content if self.target_file else ""
                print(f"ğŸŒ› OCRæ–‡æœ¬é•¿åº¦: {len(ocr_text)}")
                knowledge_base_info = ""
                # å¦‚æœæœ‰ç”¨æˆ·æé—®ï¼Œå°è¯•æ£€ç´¢çŸ¥è¯†åº“
                if user_question and user_question.strip():
                    # å¦‚æœæ˜¯chatæ¨¡å¼ï¼Œä¸æ¶‰åŠçŸ¥è¯†åº“æŸ¥è¯¢
                    if self.intent_type != 'chat':
                        relevant_docs = self.vector.query_by_question_vector_with_filter(
                            question_vector=user_question,
                            doc_types=self.intent_type,
                            top_k=3
                        )

                        if relevant_docs and len(relevant_docs) > 0:
                            # æ„å»ºçŸ¥è¯†åº“ä¸Šä¸‹æ–‡
                            knowledge_context = build_simple_context(relevant_docs)
                            knowledge_base_info = knowledge_context
                            print(f"ğŸ¯ çŸ¥è¯†åº“æ£€ç´¢åˆ° {len(relevant_docs)} æ¡ç›¸å…³ä¿¡æ¯")
                    prompt_muti_model = prompt_setting.image_word_qa_template_ocr.format(
                        question=user_question,
                        ocr_text=ocr_text,
                        knowledge_base=knowledge_base_info
                    )

                else:
                    # æ²¡æœ‰ç”¨æˆ·é—®é¢˜ï¼Œä½¿ç”¨çº¯å›¾ç‰‡åˆ†ææç¤ºè¯
                    prompt_muti_model = "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚"

                # è·å–å›¾æ–‡åˆ†æç»“æœ
                result_content = await self.llava_get_content(
                    prompt_muti_model,
                    image_byte_content
                )

                # æ„å»ºsystemæ¶ˆæ¯ï¼ˆåŒ…å«OCRå’ŒçŸ¥è¯†åº“ï¼‰
                system_message_parts = []

                if ocr_text:
                    # é™åˆ¶OCRé•¿åº¦ï¼Œé¿å…è¿‡é•¿
                    if len(ocr_text) > 2000:
                        ocr_preview = ocr_text[:2000] + "...[åé¢å†…å®¹å·²çœç•¥]"
                    else:
                        ocr_preview = ocr_text
                    system_message_parts.append(f"<ocr>ã€å›¾ç‰‡OCRæ–‡æœ¬å†…å®¹ã€‘\n{ocr_preview}")

                if knowledge_base_info:
                    system_message_parts.append(f"<ocr>ã€ç›¸å…³çŸ¥è¯†åº“ä¿¡æ¯ã€‘\n{knowledge_base_info}")

                # åˆ›å»ºåŒ…å«systemæ¶ˆæ¯å’ŒLLaVAç»“æœçš„æ¶ˆæ¯æ•°ç»„
                response_messages = []

                # å¦‚æœæœ‰systemæ¶ˆæ¯å†…å®¹ï¼Œæ·»åŠ åˆ°response_messages
                if system_message_parts:
                    system_message = "\n\n".join(system_message_parts)
                    system_message += "\n\n<ocr>"
                    response_messages.append({"role": "system", "content": system_message})

                # æ·»åŠ LLaVAçš„assistantæ¶ˆæ¯
                response_messages.append({"role": "assistant", "content": result_content})

                # æµå¼è¿”å›æ‰€æœ‰æ¶ˆæ¯
                for message in response_messages:
                    # å¦‚æœæ˜¯systemæ¶ˆæ¯ï¼Œæ·»åŠ ä¸€ä¸ªæ ‡è®°è®©å‰ç«¯çŸ¥é“è¿™æ˜¯system
                    if message["role"] == "system":
                        # å¯ä»¥æ·»åŠ ä¸€ä¸ªç‰¹æ®Šæ ‡è®°ï¼Œæ¯”å¦‚"__system__": true
                        data = {
                            "choices": [{"delta": {"content": message["content"]}}],
                            "role": "system"
                        }
                    else:
                        data = {"choices": [{"delta": {"content": message["content"]}}]}

                # æµå¼è¿”å›
                    chunks = prue_image_chunks(message["content"])
                    for chunk in chunks:
                        if not chunk.strip():
                            continue
                        # æ›´æ–°chunkå†…å®¹
                        if message["role"] == "system":
                            chunk_data = {
                                "choices": [{"delta": {"content": chunk + " "}}],
                                "role": "system"
                            }
                        else:
                            chunk_data = {"choices": [{"delta": {"content": chunk + " "}}]}

                        yield f"data: {json.dumps(chunk_data)}\n\n"

                yield "data: [DONE]\n\n"
                return

        except Exception as e:
            print(f"âŒ å›¾ç‰‡åˆ†æå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            error_data = json.dumps({"error": str(e)})
            yield f"data: {error_data}\n\n"
            yield "data: [DONE]\n\n"

    def store_document_to_vector(self, chunks, doc_type):
        try:
            print(f"ğŸš€ å…±æœ‰{len(chunks)} è¿›è¡Œä¿å­˜ï¼Œæ–‡æ¡£ç±»å‹: {doc_type}")
            for i, chunk in enumerate(chunks):
                if hasattr(chunk, 'metadata'):
                    chunk.metadata['doc_type'] = doc_type
                else:
                    chunk.metadata = {'doc_type': doc_type}
            ids = self.vector.add_document_to_vector(chunks)
            print(f" stored {self.file_name} documents successfully")
            return ids
        except Exception as e:
                print(f" stored {self.file_name} documents failed: {str(e)}")
                raise e

    def del_knowledge_item(self, ids):
        corpus_ids = self.collation_ids(ids)

        result = []
        try:
            for corpus_id in corpus_ids:
                res = self.vector.delete_document(corpus_id)
                result.append(res)

        except Exception as e:
            print(f"åˆ é™¤å¤±è´¥å‘é‡æ•°æ®åº“æ•°æ®: {str(e)}")
            raise e

        if None in result:
            return None
        else:
            return True

    def clear_all_documents(self):
        self.vector.clear_collection()

    def question_query_from_vector(self):
        """
        é€»è¾‘ï¼šç›´æ¥åŸºäºç”¨æˆ·çš„æ„å›¾åˆ†æï¼ŒåæœŸå¯èå…¥æ™ºèƒ½åˆ†æï¼Œä½†æ˜¯å¤æ‚æ€§å’Œåˆ†æè´¨é‡é—®é¢˜æœ‰å›°éš¾
        """
        results = self.vector.query_by_question_vector_with_filter(
                question_vector=self.question,
                doc_types=self.intent_type,
                top_k=8
            )
        if results and len(results) > 0:
            return results
        else:
            print(f"âš ï¸ è¿‡æ»¤æŸ¥è¯¢æ— ç»“æœï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ç±»å‹çš„å†…å®¹")
            return []

    def get_chunk_doc(self, target_file, clear_chunks=False):
        try:
            print(f"ğŸš€ start split {self.file_name}")
            splitter_chunks = TextSplitter().split_document(target_file)

            if clear_chunks:
                chunks = self.clear_data(splitter_chunks)
            else:
                chunks = splitter_chunks
            return chunks
        except Exception as e:
            print(f" split error: {e}")
            raise e

    async def stream_context_from_docs(self, documents):
        """æµå¼ç”Ÿæˆä¸Šä¸‹æ–‡ - æ­£ç¡®çš„å¤šè½®å¯¹è¯å¤„ç†"""
        llm_messages = []

        # 1. å¦‚æœæœ‰çŸ¥è¯†åº“ä¿¡æ¯ï¼Œä½œä¸ºsystemæ¶ˆæ¯
        if documents:
            context_str = build_simple_context(documents)
            system_content = prompt_setting.knowledge_history_template.format(context_str=context_str)
            if self.intent_type == 'resume':
                system_content = prompt_setting.knowledge_history_resume_template.format(context_str=context_str)

            llm_messages.append({
                "role": "system",
                "content": system_content
            })

        # 2. æ£€æŸ¥æ˜¯å¦æ˜¯æŒ‡ä»£å›¾ç‰‡çš„é—®é¢˜
        is_image_reference = False
        image_reference_text = ""

        if self.messages and len(self.messages) >= 2:
            current_question = self.question or ""
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡ç›¸å…³çš„æŒ‡ä»£è¯
            image_keywords = ["å›¾", "å›¾ç‰‡", "ç…§ç‰‡", "æˆªå›¾", "ç”»é¢", "å›¾åƒ", "photo", "image"]
            has_image_keyword = any(keyword in current_question for keyword in image_keywords)

            if has_image_keyword:
                # æŸ¥æ‰¾æœ€è¿‘çš„å›¾ç‰‡æè¿°
                for i in range(len(self.messages) - 2, -1, -1):  # ä»å€’æ•°ç¬¬äºŒæ¡å¾€å‰æ‰¾
                    msg = self.messages[i]
                    if isinstance(msg, dict):
                        # æ£€æŸ¥æ˜¯å¦æ˜¯assistantçš„å›å¤ä¸”åŒ…å«å›¾ç‰‡æè¿°ç‰¹å¾
                        content = msg.get("content", "")
                        if ("è¿™æ˜¯ä¸€å¼ " in content or "ç…§ç‰‡" in content or
                                "åœºæ™¯" in content or "ç”»é¢" in content):
                            is_image_reference = True
                            image_reference_text = content
                            break
        if self.messages:
            # ç¡®ä¿æ ¼å¼æ­£ç¡®
            for msg in self.messages:
                normalized_msg = {}
                # è½¬æ¢role
                if "type" in msg:
                    normalized_msg["role"] = "user" if msg["type"] == "user" else "assistant"
                elif "role" in msg:
                    normalized_msg["role"] = msg["role"]
                else:
                    normalized_msg["role"] = "user"  # é»˜è®¤

                # ç¡®ä¿contentå­˜åœ¨
                if "content" in msg:
                    normalized_msg["content"] = msg["content"]
                elif "text" in msg:
                    normalized_msg["content"] = msg["text"]
                else:
                    normalized_msg["content"] = ""

                # åªæ·»åŠ æœ‰å†…å®¹çš„message
                if normalized_msg["content"].strip():
                    llm_messages.append(normalized_msg)

        if is_image_reference and image_reference_text:
            # åœ¨æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯åæ·»åŠ ç³»ç»Ÿæç¤º
            for i in range(len(llm_messages) - 1, -1, -1):
                if llm_messages[i].get("role") == "user":
                    # ä¿®æ”¹å½“å‰ç”¨æˆ·é—®é¢˜ï¼Œæ˜ç¡®å¼•ç”¨å›¾ç‰‡æè¿°
                    original_content = llm_messages[i]["content"]
                    enhanced_content = f"""
                            {original_content}
                        ï¼ˆæç¤ºï¼šæ ¹æ®ä¹‹å‰çš„å¯¹è¯ï¼Œå›¾ç‰‡æè¿°ä¸ºï¼š{image_reference_text[:200]}...è¯·åŸºäºè¿™ä¸ªå›¾ç‰‡æè¿°å›ç­”ã€‚ï¼‰"""

                    llm_messages[i]["content"] = enhanced_content
                    break
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        try:
            # è°ƒç”¨æµå¼LLMï¼Œä¼ é€’æ­£ç¡®çš„messagesæ•°ç»„
            async for chunk in stream_llm_response(llm_messages):
                if chunk:
                    yield chunk

            end_time = time.time()
            print(f"âœ… æµå¼ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")

        except Exception as e:
            print(f"âŒ æµå¼ç”Ÿæˆå¼‚å¸¸: {e}")
            error_data = json.dumps({"error": str(e)})
            yield f"data: {error_data}\n\n"
            yield "data: [DONE]\n\n"

    async def upload_infor_to_vector(self):
        try:
            if self.file_type != 'image':
                print(f" âœ… å¼€å§‹è¿›è¡Œä¿å­˜çŸ¥è¯†åº“æ“ä½œ, ä¸Šä¼ çš„çŸ¥è¯†ç±»å‹{self.doc_type}")
                chunks = self.get_chunk_doc(self.target_file)
                stored_ids = self.store_document_to_vector(chunks, self.doc_type)
                return stored_ids
            else:
                print(f"ä¸èƒ½ä¸Šä¼ å›¾ç‰‡")
                pass
        except Exception as e:
            print(f"âŒå­˜å‚¨å‘é‡æ•°æ®åº“å¤±è´¥ {str(e)}")

    def clear_data(self, chunks):
        all_rag_chunks = []

        for j, chunk in enumerate(chunks):
            cur_document = Document(
                page_content=chunk.page_content,
                metadata={
                    "source":self.file_name,
                    "chunk":j
                }
            )
            all_rag_chunks.append(cur_document)
        clearner = AdvancedTextCleaner()
        cleaned_chunks = clearner.clean_documents(all_rag_chunks)
        print(f" from {len(all_rag_chunks)} remove {len(cleaned_chunks)} ")
        return cleaned_chunks

    def collation_ids(self, ids):
        data_dict = dict(ids)
        corpus_ids = data_dict.get("corpus_ids", [])
        return corpus_ids

    def dev_env_test_api(self):
        self.vector.verify_doc_type_storage()
        # éªŒè¯ç‰¹å®šç±»å‹
        # self.vector.verify_doc_type_storage("resume")
        # self.vector.verify_doc_type_storage("code")

    # async def run_by_web(self):
    #     print(f"ğŸš€ Rag started at {datetime.datetime.now()} ")
    #     try:
    #         loader = DocumentLoader(urls=["https://tailwindcss.com/docs/installation/using-vite"])
    #         docs = await loader.load()
    #         text = "\n".join([doc.page_content for doc in docs])
    #         chunks = TextSplitter().split_text(text)
    #         print(f" åˆ†å‰²å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
    #         return chunks
    #     except Exception as e:
    #         print(f" æ‰§è¡Œå¤±è´¥: {e}")
    #         raise

    # async def main():
    #     app = RagService()
    #     res = await app.run_rag()
    #     for i, chunk in enumerate(res):
    #         print(f"--- åˆ†å— {i + 1} ---\n{chunk[:200]}...")
    #
    # asyncio.run(main())

