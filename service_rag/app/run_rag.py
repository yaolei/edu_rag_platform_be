from typing import Optional, List, Dict
from fastapi import UploadFile
import asyncio
import time
import json
from pathlib import Path
from service_rag.app.embedding.embedding_data import EmbeddingData
from service_rag.app.prompt.prompt import prompt_setting
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document
from service_rag.app.document_operation.document_loader import DocumentLoader
from service_rag.app.text_splitter.text_split import TextSplitter
from service_rag.app.vector.vector_store import VectorStore
from service_rag.app.llm_model.contect_llm import  connect_text_llm, analyze_with_image, stream_llm_response
from service_rag.app.text_splitter.advanced_text_cleaner import AdvancedTextCleaner
from service_rag.app.service.gen_util import switch_correct_prompt, build_simple_context, prue_image_chunks

class RagService:
    def __init__(self):
        self.prompt = PromptTemplate(input_variables=['context', 'question'],
                                     template=prompt_setting.rag_template)
        self.embedding_type = None
        self.upload_file = None
        self.file_name = None
        self.target_file = None
        self.embeddings = None
        self.vector = None
        self.question = None
        self.file_type = None
        self.if_files = None
        self.doc_type = None
        self.mutil_files = []

        self.image_binary_data = None
        self.conversation_id = None
        self.messages = []
        self.last_doc_types = []  # ä¿å­˜ä¸Šæ¬¡çš„æ„å›¾åˆ†æç»“æœ
        # self.conversation_topic = None

    @classmethod
    async def create(cls, upload_file: List[UploadFile]=None, embedding_type='questions', doc_type="document",
                                                             conversation_id: Optional[str] = None,
                                                             messages: Optional[List[Dict]] = None, **kwargs):
        instance = cls()
        await instance.initialize(upload_file, embedding_type, doc_type, conversation_id=conversation_id,
                                                                                      messages=messages,
                                                                                      **kwargs)
        return instance

    async def initialize(self, upload_file: List[UploadFile]=None, embedding_type='questions', doc_type="document",
                         conversation_id: Optional[str] = None,
                         messages: Optional[List[Dict]] = None, **kwargs):
        self.embedding_type = embedding_type

        self.upload_file = upload_file
        self.doc_type = doc_type

        self.conversation_id = conversation_id  # æ–°å¢
        self.messages = messages or []
        self.question = ""
        if self.messages:
            for msg in reversed(self.messages):
                if msg.get("role") == "user":
                    self.question = msg.get("content", "").strip()
                    break
            if self.question:
                print(f"ğŸ¯ ä»messagesä¸­æå–çš„é—®é¢˜: {self.question}")

        self.embeddings = EmbeddingData(embedding_type=embedding_type)
        self.vector = VectorStore(embedding_function=self.embeddings)

        if self.messages:
            print(f"ğŸ“š æ¥æ”¶åˆ° {len(self.messages)} æ¡å†å²æ¶ˆæ¯")

        if not upload_file:  # æ— æ–‡ä»¶
            pass
        elif len(upload_file) == 1:
            self.if_files = False
            self.file_name = upload_file[0].filename or "unknown file"
            path_obj = Path(self.file_name)

            try:
                if upload_file[0].content_type and upload_file[0].content_type.startswith('image/'):
                    content = await upload_file[0].read()
                    self.image_binary_data = content
                    # é‡ç½®æ–‡ä»¶æŒ‡é’ˆï¼Œä»¥ä¾¿ DocumentLoader å¯ä»¥è¯»å–
                    await upload_file[0].seek(0)

                document_loader = DocumentLoader(upload_file[0])
                self.target_file = await document_loader.load()
                document_loader.cleanup_temp_resources()
                if self.target_file and self.target_file[0].page_content == '':
                    self.target_file = None
                else:
                    if 'document_loader' in locals():
                        document_loader.cleanup_temp_resources()
                    self.file_type = document_loader._detect_document_type()
                    return False
            except Exception as e:
                print(f"âŒ embedding module error: {str(e)}")
                raise e
        else:
            self.if_files = True
            for f in upload_file:
                document_loader_muti_file = DocumentLoader(f)
                self.mutil_files.append(await document_loader_muti_file.load())
                document_loader_muti_file.cleanup_temp_resources()
            print(f"ğŸ¯ {self.mutil_files} ğŸ¯")


    # async def llava_get_content(self, prompt_sentence, image_bytes, is_text_image):
    #     prompt_sentence = prompt_sentence.strip()
    #
    #     if self.messages and len(self.messages) > 0:
    #         history_text = ""
    #         for msg in self.messages[-5:]:  # åªå–æœ€è¿‘5æ¡æ¶ˆæ¯
    #             role = "ç”¨æˆ·" if msg.get("role") == "user" else "åŠ©æ‰‹"
    #             content = msg.get("content", "")
    #             history_text += f"{role}: {content}\n"
    #
    #         enhanced_prompt = f"ã€å¯¹è¯å†å²ã€‘\n{history_text}\nã€å½“å‰ä»»åŠ¡ã€‘\n"
    #     else:
    #         enhanced_prompt = ""
    #
    #     if not is_text_image:
    #         if self.question:
    #             llava_prompt = prompt_setting.pure_image_qa_template.format(question=self.question)
    #             print(f"ğŸ¦ ç”¨æˆ·æé—®: {llava_prompt}")
    #         else:
    #             llava_prompt = prompt_sentence
    #             print(f"ğŸ¦ ç”¨æˆ·æœªæé—®ï¼Œè‡ªåŠ¨ç”Ÿæˆå›¾ç‰‡æè¿°{llava_prompt}")
    #     else:
    #         llava_prompt = prompt_sentence
    #         print(f"ğŸ¦ åŸå§‹æç¤ºè¯ï¼Œè‡ªåŠ¨ç”Ÿæˆå›¾ç‰‡æè¿°{llava_prompt}")
    #
    #     # å¦‚æœæœ‰å†å²è®°å½•ï¼Œæ·»åŠ åˆ°æç¤ºè¯ä¸­
    #     if self.messages and len(self.messages) > 0:
    #         llava_prompt = enhanced_prompt + llava_prompt
    #         print(f"ğŸ¯ ä½¿ç”¨ä¸Šä¸‹æ–‡å¢å¼ºå›¾ç‰‡åˆ†æ")
    #
    #     final_answer = await analyze_with_image(
    #         image_bytes=image_bytes,
    #         question=llava_prompt,
    #         messages=self.messages
    #     )
    #
    #     if isinstance(final_answer, dict) and 'content' in final_answer:
    #         result_content = final_answer['content'].strip()
    #     else:
    #         result_content = str(final_answer).strip()
    #
    #     return result_content

    # async def analyse_image_information(self):
    #     """
    #     1. ä½¿ç”¨ä¸“ä¸šæç¤ºè¯è®©LLaVAåˆ†æå›¾ç‰‡
    #     2. åˆ†æç”¨æˆ·é—®é¢˜æ„å›¾
    #     3. æ ¹æ®æ„å›¾å†³å®šæ˜¯å¦æŸ¥è¯¢çŸ¥è¯†åº“
    #     4. ä½¿ç”¨ä¸“ä¸šå›¾ç‰‡é—®ç­”æ¨¡æ¿ç”Ÿæˆæœ€ç»ˆå›ç­”
    #     """
    #     try:
    #         print(f"ğŸ¦ å¤„ç†æ–‡ä»¶: {self.file_name}")
    #         image_byte_content = self.image_binary_data
    #         print(f"âœ… ä½¿ç”¨ç¼“å­˜çš„å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®: {len(image_byte_content)} å­—èŠ‚")
    #
    #         # è·å–å¯¹è¯å†å²
    #         history_str = ""
    #         if self.messages:
    #             for msg in self.messages:
    #                 role = "ç”¨æˆ·" if msg.get("role") == "user" else "åŠ©æ‰‹"
    #                 content = msg.get("content", "")
    #                 history_str += f"{role}: {content}\n"
    #
    #         # çº¯å›¾ç‰‡
    #         is_pure_image = not self.target_file
    #         if is_pure_image:
    #             print("ğŸ¯ è¿›å…¥çº¯å›¾ç‰‡åˆ†æåˆ†æ”¯")
    #             # è·å–çº¯å›¾ç‰‡åˆ†æç»“æœ
    #             result_content = await self.llava_get_content(
    #                 prompt_setting.prue_image_analysis_template,
    #                 image_byte_content,
    #                 False
    #             )
    #             print(f"ğŸ“Š è·å–åˆ°çº¯å›¾ç‰‡åˆ†æç»“æœï¼Œé•¿åº¦: {len(result_content)}")
    #
    #             if self.messages and len(self.messages) > 0:
    #                 # æ„å»ºå¸¦ä¸Šä¸‹æ–‡çš„æç¤ºè¯
    #                 conversation_prompt = prompt_setting.image_conversation_template.replace(
    #                     '{history}', history_str
    #                 ).replace(
    #                     '{image_analysis}', result_content
    #                 ).replace(
    #                     '{question}', self.question if self.question else "è¯·æè¿°è¿™å¼ å›¾ç‰‡"
    #                 )
    #
    #                 # ä½¿ç”¨æ–°çš„æç¤ºè¯é‡æ–°åˆ†æ
    #                 enhanced_result = await self.llava_get_content(
    #                     conversation_prompt,
    #                     image_byte_content,
    #                     False
    #                 )
    #                 result_content = enhanced_result
    #                 print(f"ğŸ¯ ä½¿ç”¨ä¸Šä¸‹æ–‡å¢å¼ºåˆ†æï¼Œæ–°é•¿åº¦: {len(result_content)}")
    #
    #             chunks = prue_image_chunks(result_content)
    #             # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæµå¼è¿”å› - ä½¿ç”¨å¼‚æ­¥æ–¹å¼
    #             import json
    #             for i, chunk in enumerate(chunks):
    #                 if not chunk.strip():
    #                     continue
    #
    #                 data = {
    #                     "choices": [{"delta": {"content": chunk + " "}}]
    #                 }
    #                 yield f"data: {json.dumps(data)}\n\n"
    #
    #                 # æ ¹æ®chunké•¿åº¦åŠ¨æ€è°ƒæ•´å»¶è¿Ÿ
    #                 delay = min(0.15, max(0.05, len(chunk) / 300))
    #                 await asyncio.sleep(delay)
    #
    #             yield "data: [DONE]\n\n"
    #             return
    #
    #         else:
    #             # ========== æƒ…å†µ1ï¼šå›¾æ–‡å¤„ç† ==========
    #             print(f"ğŸ¦ å¼€å§‹åˆ†æå›¾åƒä¿¡æ¯ï¼Œé—®é¢˜: {self.question} ğŸ¦")
    #
    #             # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦è¾“å…¥æé—®ä¿¡æ¯
    #             analyse_text_image = await self.llava_get_content(
    #                 prompt_setting.rag_image_analysis_template,
    #                 image_byte_content,
    #                 True
    #             )
    #
    #             if self.messages and len(self.messages) > 0:
    #                 conversation_prompt = prompt_setting.image_conversation_template.replace(
    #                     '{history}', history_str
    #                 ).replace(
    #                     '{image_analysis}', analyse_text_image
    #                 ).replace(
    #                     '{question}', self.question if self.question else "è¯·åˆ†æå›¾ç‰‡å†…å®¹"
    #                 )
    #
    #                 enhanced_result = await self.llava_get_content(
    #                     conversation_prompt,
    #                     image_byte_content,
    #                     True
    #                 )
    #                 analyse_text_image = enhanced_result
    #
    #             if not self.question or self.question.strip() == "":
    #                 print("ğŸ¯ æ²¡æœ‰ç”¨æˆ·é—®é¢˜ï¼Œç›´æ¥è¿”å›å›¾ç‰‡åˆ†æç»“æœ")
    #                 # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæµå¼è¿”å›
    #                 import json
    #                 chunk_size = 50
    #                 total_chunks = (len(analyse_text_image) + chunk_size - 1) // chunk_size
    #
    #                 for i in range(0, len(analyse_text_image), chunk_size):
    #                     chunk = analyse_text_image[i:i + chunk_size]
    #                     data = {
    #                         "choices": [{"delta": {"content": chunk}}]
    #                     }
    #                     print(f"ğŸ“¤ å‘é€ç¬¬ {i // chunk_size + 1}/{total_chunks} ä¸ª chunkï¼Œé•¿åº¦: {len(chunk)}")
    #                     yield f"data: {json.dumps(data)}\n\n"
    #                     await asyncio.sleep(0.01)
    #
    #                 yield "data: [DONE]\n\n"
    #
    #             else:
    #                 print(f"ğŸ¯ æœ‰ç”¨æˆ·é—®é¢˜ï¼Œè¿›è¡Œæ„å›¾åˆ†æå’ŒçŸ¥è¯†åº“æŸ¥è¯¢")
    #                 image_description = analyse_text_image
    #                 ocr_text = self.target_file[0].page_content
    #                 intent_analysis_prompt = prompt_setting.image_intent_prompt.format(
    #                     image_description=image_description,
    #                     ocr_text=ocr_text
    #                 )
    #                 doc_types = self.analyze_intent_with_llm(intent_analysis_prompt)
    #                 print(f"ğŸˆ¶ é—®é¢˜çš„å›¾æ–‡ç±»å‹ç»“æœæ˜¯: {doc_types}")
    #
    #                 if len(doc_types) > 0:
    #                     print(f"ğŸˆ¶ çŸ¥è¯†åº“åŒ…å«é—®é¢˜ç±»å‹ï¼Œå¼€å§‹è¿›è¡ŒçŸ¥è¯†åº“æŸ¥è¯¢")
    #                     relevant_docs = self.vector.query_by_question_vector_with_filter(
    #                         question_vector=self.question,
    #                         doc_types=doc_types,
    #                         top_k=5
    #                     )
    #
    #                     if len(relevant_docs) > 0:
    #                         print(f"ğŸ¯ çŸ¥è¯†åº“æœ‰ç›¸å…³ä¿¡æ¯ï¼Œå¼€å§‹æ™ºèƒ½èåˆçŸ¥è¯†åº“ä¿¡æ¯å’Œç”¨æˆ·é—®é¢˜")
    #                         final_prompt_for_text_model = switch_correct_prompt(
    #                             self.question,
    #                             doc_types[0],
    #                             image_description,
    #                             relevant_docs,
    #                             ocr_text
    #                         )
    #
    #                         # è®°å½•å¼€å§‹æ—¶é—´
    #                         start_time = time.time()
    #                         print(f"ğŸ”„ å›¾ç‰‡æ–‡æ¨¡å¼:å¼€å§‹æµå¼ç”Ÿæˆï¼Œprompté•¿åº¦: {len(final_prompt_for_text_model)}")
    #
    #                         # è°ƒç”¨æµå¼LLM
    #                         chunk_count = 0
    #                         llm_messages = self.messages.copy() if self.messages else []
    #                         llm_messages.append({"role": "user", "content": final_prompt_for_text_model})
    #                         async for chunk in stream_llm_response(llm_messages):
    #                             if chunk:
    #                                 chunk_count += 1
    #                                 if chunk_count % 10 == 0:  # æ¯10ä¸ªchunkæ‰“å°ä¸€æ¬¡
    #                                     print(f"ğŸ“¤ æµå¼LLMç¬¬ {chunk_count} ä¸ª chunk")
    #                                 yield chunk
    #
    #                         # å‘é€ç»“æŸä¿¡å·
    #                         yield "data: [DONE]\n\n"
    #                         end_time = time.time()
    #                         print(f"âœ… æµå¼ç”Ÿæˆå®Œæˆï¼Œå…± {chunk_count} ä¸ª chunkï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
    #
    #                     else:
    #                         print(f"ğŸ¯ çŸ¥è¯†åº“æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œç›´æ¥è¿”å›å›¾ç‰‡åˆ†æç»“æœ")
    #                         # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæµå¼è¿”å›
    #                         import json
    #                         chunk_size = 50
    #                         total_chunks = (len(analyse_text_image) + chunk_size - 1) // chunk_size
    #
    #                         for i in range(0, len(analyse_text_image), chunk_size):
    #                             chunk = analyse_text_image[i:i + chunk_size]
    #                             data = {
    #                                 "choices": [{"delta": {"content": chunk}}]
    #                             }
    #                             print(f"ğŸ“¤ å‘é€ç¬¬ {i // chunk_size + 1}/{total_chunks} ä¸ª chunkï¼Œé•¿åº¦: {len(chunk)}")
    #                             yield f"data: {json.dumps(data)}\n\n"
    #                             await asyncio.sleep(0.01)
    #
    #                         yield "data: [DONE]\n\n"
    #
    #                 else:
    #                     print(f"ğŸ¯ æ— åŒ¹é…æ–‡æ¡£ç±»å‹ï¼Œè¿”å›å›¾ç‰‡åˆ†æç»“æœ")
    #                     # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæµå¼è¿”å›
    #                     import json
    #                     chunk_size = 50
    #                     total_chunks = (len(analyse_text_image) + chunk_size - 1) // chunk_size
    #
    #                     for i in range(0, len(analyse_text_image), chunk_size):
    #                         chunk = analyse_text_image[i:i + chunk_size]
    #                         data = {
    #                             "choices": [{"delta": {"content": chunk}}]
    #                         }
    #                         print(f"ğŸ“¤ å‘é€ç¬¬ {i // chunk_size + 1}/{total_chunks} ä¸ª chunkï¼Œé•¿åº¦: {len(chunk)}")
    #                         yield f"data: {json.dumps(data)}\n\n"
    #                         await asyncio.sleep(0.01)
    #
    #                     yield "data: [DONE]\n\n"
    #
    #     except Exception as e:
    #         import json
    #         print(f"âŒ å›¾ç‰‡åˆ†æå¼‚å¸¸: {e}")
    #         import traceback
    #         traceback.print_exc()
    #         error_data = json.dumps({"error": str(e)})
    #         yield f"data: {error_data}\n\n"
    #         yield "data: [DONE]\n\n"

    async def llava_get_content(self, prompt_sentence, image_bytes, is_text_image, user_question=""):
        """è·å–LLaVAåˆ†æç»“æœ"""
        prompt_sentence = prompt_sentence.strip()
        print(f"ğŸŒ› is_text_image: {is_text_image}")
        print(f"ğŸŒ› ç”¨æˆ·é—®é¢˜: {user_question}")

        if not is_text_image:
            # çº¯å›¾ç‰‡æ¨¡å¼
            if user_question and user_question.strip():
                # æœ‰ç”¨æˆ·æé—®ï¼Œä½¿ç”¨é—®ç­”æ¨¡æ¿
                llava_prompt = prompt_setting.pure_image_qa_template.format(question=user_question)
                print(f"ğŸ¦ çº¯å›¾ç‰‡å¸¦é—®é¢˜æé—®æ¨¡å¼")
            else:
                # æ²¡æœ‰ç”¨æˆ·æé—®ï¼Œä½¿ç”¨æè¿°æ¨¡æ¿
                llava_prompt = prompt_sentence
                print(f"ğŸ¦ çº¯å›¾ç‰‡æè¿°æ¨¡å¼")
        else:
            # å›¾æ–‡æ··åˆæ¨¡å¼ - ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„æç¤ºè¯
            llava_prompt = prompt_sentence
            print(f"ğŸ¦ å›¾æ–‡æ··åˆåˆ†ææ¨¡å¼")

        print(f"ğŸŒ› å‘é€ç»™LLaVAçš„æç¤ºè¯é•¿åº¦: {len(llava_prompt)}")

        final_answer = await analyze_with_image(
            image_bytes=image_bytes,
            question=llava_prompt,
            messages=[]  # å›¾ç‰‡å¯¹è¯ä¸ä½¿ç”¨å†å²æ¶ˆæ¯
        )

        if isinstance(final_answer, dict) and 'content' in final_answer:
            result_content = final_answer['content'].strip()
        else:
            result_content = str(final_answer).strip()

        print(f"ğŸŒ› LLaVAè¿”å›ç»“æœé•¿åº¦: {len(result_content)}")
        return result_content


    # async def analyse_image_information(self):
    #     """
    #     åˆ†æå›¾ç‰‡ä¿¡æ¯ - å›¾ç‰‡å¯¹è¯ç‹¬ç«‹å¤„ç†
    #     """
    #     try:
    #         image_byte_content = self.image_binary_data
    #         print(f"âœ… ä½¿ç”¨ç¼“å­˜çš„å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®: {len(image_byte_content)} å­—èŠ‚")
    #         # è·å–æœ€åä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    #         user_question = ""
    #         if self.messages:
    #             for msg in reversed(self.messages):
    #                 if msg.get("role") == "user":
    #                     user_question = msg.get("content", "").strip()
    #                     break
    #
    #         # çº¯å›¾ç‰‡
    #         is_pure_image = not self.target_file
    #         if is_pure_image:
    #             print("ğŸ¯ è¿›å…¥çº¯å›¾ç‰‡åˆ†æåˆ†æ”¯")
    #             # è·å–çº¯å›¾ç‰‡åˆ†æç»“æœ
    #             result_content = await self.llava_get_content(
    #                 prompt_setting.prue_image_analysis_template,
    #                 image_byte_content,
    #                 False,  # ä¸æ˜¯å›¾æ–‡æ··åˆ
    #                 user_question  # ä¼ é€’ç”¨æˆ·æé—®
    #             )
    #
    #             # å°†ç»“æœæµå¼è¿”å›
    #             chunks = prue_image_chunks(result_content)
    #             for i, chunk in enumerate(chunks):
    #                 if not chunk.strip():
    #                     continue
    #
    #                 data = {
    #                     "choices": [{"delta": {"content": chunk + " "}}]
    #                 }
    #                 yield f"data: {json.dumps(data)}\n\n"
    #
    #                 # æ ¹æ®chunké•¿åº¦åŠ¨æ€è°ƒæ•´å»¶è¿Ÿ
    #                 delay = min(0.15, max(0.05, len(chunk) / 300))
    #                 await asyncio.sleep(delay)
    #
    #             yield "data: [DONE]\n\n"
    #             return
    #
    #         else:
    #             # ========== æƒ…å†µ1ï¼šå›¾æ–‡å¤„ç† ==========
    #             print(f"ğŸ¦ å¼€å§‹åˆ†æå›¾æ–‡ä¿¡æ¯")
    #
    #             # è·å–å›¾æ–‡åˆ†æç»“æœ
    #             analyse_text_image = await self.llava_get_content(
    #                 prompt_setting.rag_image_analysis_template,
    #                 image_byte_content,
    #                 True,
    #                 user_question
    #             )
    #
    #             if not self.question or self.question.strip() == "":
    #                 print("ğŸ¯ æ²¡æœ‰ç”¨æˆ·é—®é¢˜ï¼Œç›´æ¥è¿”å›å›¾ç‰‡åˆ†æç»“æœ")
    #                 # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæµå¼è¿”å›
    #                 chunk_size = 50
    #                 total_chunks = (len(analyse_text_image) + chunk_size - 1) // chunk_size
    #
    #                 for i in range(0, len(analyse_text_image), chunk_size):
    #                     chunk = analyse_text_image[i:i + chunk_size]
    #                     data = {
    #                         "choices": [{"delta": {"content": chunk}}]
    #                     }
    #                     yield f"data: {json.dumps(data)}\n\n"
    #                     await asyncio.sleep(0.01)
    #
    #                 yield "data: [DONE]\n\n"
    #
    #             else:
    #                 print(f"ğŸ¯ æœ‰ç”¨æˆ·é—®é¢˜ï¼Œè¿›è¡Œæ„å›¾åˆ†æå’ŒçŸ¥è¯†åº“æŸ¥è¯¢")
    #                 image_description = analyse_text_image
    #                 ocr_text = self.target_file[0].page_content
    #                 intent_analysis_prompt = prompt_setting.image_intent_prompt.format(
    #                     image_description=image_description,
    #                     ocr_text=ocr_text
    #                 )
    #                 doc_types = self.analyze_intent_with_llm(intent_analysis_prompt)
    #                 print(f"ğŸˆ¶ é—®é¢˜çš„å›¾æ–‡ç±»å‹ç»“æœæ˜¯: {doc_types}")
    #
    #                 if len(doc_types) > 0:
    #                     print(f"ğŸˆ¶ çŸ¥è¯†åº“åŒ…å«é—®é¢˜ç±»å‹ï¼Œå¼€å§‹è¿›è¡ŒçŸ¥è¯†åº“æŸ¥è¯¢")
    #                     relevant_docs = self.vector.query_by_question_vector_with_filter(
    #                         question_vector=self.question,
    #                         doc_types=doc_types,
    #                         top_k=5
    #                     )
    #
    #                     if len(relevant_docs) > 0:
    #                         print(f"ğŸ¯ çŸ¥è¯†åº“æœ‰ç›¸å…³ä¿¡æ¯ï¼Œå¼€å§‹æ™ºèƒ½èåˆçŸ¥è¯†åº“ä¿¡æ¯å’Œç”¨æˆ·é—®é¢˜")
    #                         final_prompt_for_text_model = switch_correct_prompt(
    #                             self.question,
    #                             doc_types[0],
    #                             image_description,
    #                             relevant_docs,
    #                             ocr_text
    #                         )
    #
    #                         # è®°å½•å¼€å§‹æ—¶é—´
    #                         start_time = time.time()
    #                         print(f"ğŸ”„ å›¾ç‰‡æ–‡æ¨¡å¼:å¼€å§‹æµå¼ç”Ÿæˆï¼Œprompté•¿åº¦: {len(final_prompt_for_text_model)}")
    #
    #                         # è°ƒç”¨æµå¼LLM - å›¾ç‰‡å¯¹è¯ä¸ä½¿ç”¨å†å²æ¶ˆæ¯
    #                         chunk_count = 0
    #                         llm_messages = [{"role": "user", "content": final_prompt_for_text_model}]
    #                         async for chunk in stream_llm_response(llm_messages):
    #                             if chunk:
    #                                 chunk_count += 1
    #                                 if chunk_count % 10 == 0:  # æ¯10ä¸ªchunkæ‰“å°ä¸€æ¬¡
    #                                     print(f"ğŸ“¤ æµå¼LLMç¬¬ {chunk_count} ä¸ª chunk")
    #                                 yield chunk
    #
    #                         # å‘é€ç»“æŸä¿¡å·
    #                         yield "data: [DONE]\n\n"
    #                         end_time = time.time()
    #                         print(f"âœ… æµå¼ç”Ÿæˆå®Œæˆï¼Œå…± {chunk_count} ä¸ª chunkï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
    #
    #                     else:
    #                         print(f"ğŸ¯ çŸ¥è¯†åº“æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¿”å›å›¾ç‰‡åˆ†æç»“æœ")
    #                         # å°†å›¾ç‰‡åˆ†æç»“æœæµå¼è¿”å›
    #                         chunk_size = 50
    #                         total_chunks = (len(analyse_text_image) + chunk_size - 1) // chunk_size
    #
    #                         for i in range(0, len(analyse_text_image), chunk_size):
    #                             chunk = analyse_text_image[i:i + chunk_size]
    #                             data = {
    #                                 "choices": [{"delta": {"content": chunk}}]
    #                             }
    #                             yield f"data: {json.dumps(data)}\n\n"
    #                             await asyncio.sleep(0.01)
    #
    #                         yield "data: [DONE]\n\n"
    #
    #                 else:
    #                     print(f"ğŸ¯ æ— åŒ¹é…æ–‡æ¡£ç±»å‹ï¼Œè¿”å›å›¾ç‰‡åˆ†æç»“æœ")
    #                     # å°†å›¾ç‰‡åˆ†æç»“æœæµå¼è¿”å›
    #                     chunk_size = 50
    #                     total_chunks = (len(analyse_text_image) + chunk_size - 1) // chunk_size
    #
    #                     for i in range(0, len(analyse_text_image), chunk_size):
    #                         chunk = analyse_text_image[i:i + chunk_size]
    #                         data = {
    #                             "choices": [{"delta": {"content": chunk}}]
    #                         }
    #                         yield f"data: {json.dumps(data)}\n\n"
    #                         await asyncio.sleep(0.01)
    #
    #                     yield "data: [DONE]\n\n"
    #
    #     except Exception as e:
    #         print(f"âŒ å›¾ç‰‡åˆ†æå¼‚å¸¸: {e}")
    #         import traceback
    #         traceback.print_exc()
    #         error_data = json.dumps({"error": str(e)})
    #         yield f"data: {error_data}\n\n"
    #         yield "data: [DONE]\n\n"

    async def analyse_image_information(self):
        """
        åˆ†æå›¾ç‰‡ä¿¡æ¯ - ç»Ÿä¸€ä½¿ç”¨messageæ•°ç»„æ¨¡å¼
        """
        try:
            image_byte_content = self.image_binary_data
            print(f"âœ… ä½¿ç”¨ç¼“å­˜çš„å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®: {len(image_byte_content)} å­—èŠ‚")

            # è·å–æœ€åä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            user_question = ""
            if self.messages:
                for msg in reversed(self.messages):
                    if msg.get("role") == "user":
                        user_question = msg.get("content", "").strip()
                        break

            print(f"ğŸŒ› ç”¨æˆ·é—®é¢˜: '{user_question}'")

            # çº¯å›¾ç‰‡
            is_pure_image = not self.target_file
            if is_pure_image:
                print("ğŸ¯ è¿›å…¥çº¯å›¾ç‰‡åˆ†æåˆ†æ”¯")

                # æƒ…å†µ1: æ— ç”¨æˆ·æé—® - ç›´æ¥è¿”å›å›¾ç‰‡æè¿°
                if not user_question or user_question.strip() == "":
                    print("ğŸ¯ çº¯å›¾ç‰‡æ— æé—®ï¼Œç›´æ¥è¿”å›æè¿°")
                    result_content = await self.llava_get_content(
                        prompt_setting.prue_image_analysis_template,
                        image_byte_content,
                        False,  # ä¸æ˜¯å›¾æ–‡æ··åˆ
                        ""  # æ— ç”¨æˆ·æé—®
                    )

                    # å°†ç»“æœæµå¼è¿”å›
                    chunks = prue_image_chunks(result_content)
                    for chunk in chunks:
                        if not chunk.strip():
                            continue
                        data = {"choices": [{"delta": {"content": chunk + " "}}]}
                        yield f"data: {json.dumps(data)}\n\n"
                        await asyncio.sleep(min(0.15, max(0.05, len(chunk) / 300)))

                    yield "data: [DONE]\n\n"
                    return

                # æƒ…å†µ2: æœ‰ç”¨æˆ·æé—® - ä½¿ç”¨messageæ•°ç»„æ¨¡å¼
                else:
                    print("ğŸ¯ çº¯å›¾ç‰‡æœ‰æé—®ï¼Œä½¿ç”¨messageæ•°ç»„æ¨¡å¼")
                    # è·å–å›¾ç‰‡åˆ†æç»“æœ
                    image_description = await self.llava_get_content(
                        prompt_setting.prue_image_analysis_template,
                        image_byte_content,
                        False,  # ä¸æ˜¯å›¾æ–‡æ··åˆ
                        user_question  # ä¼ é€’ç”¨æˆ·æé—®
                    )

                    # æ„å»ºsystemæ¶ˆæ¯
                    system_message = f"ã€å›¾ç‰‡åˆ†æç»“æœã€‘\n{image_description}\n\nè¯·æ ¹æ®å›¾ç‰‡å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"

                    # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯æ•°ç»„
                    llm_messages = [{"role": "system", "content": system_message}]

                    # æ·»åŠ å†å²æ¶ˆæ¯ï¼ˆå‰ç«¯å·²é™åˆ¶æ•°é‡ï¼‰
                    if self.messages:
                        for msg in self.messages:
                            normalized_msg = {"role": msg.get("role", "user"), "content": msg.get("content", "")}
                            if normalized_msg["content"].strip():
                                llm_messages.append(normalized_msg)

                    print(f"ğŸ”„ çº¯å›¾ç‰‡messageæ¨¡å¼: æ¶ˆæ¯æ€»æ•° {len(llm_messages)}")

                    # è°ƒç”¨æµå¼LLM
                    async for chunk in stream_llm_response(llm_messages):
                        yield chunk

                    yield "data: [DONE]\n\n"
                    return

            else:
                # ========== å›¾æ–‡å¤„ç†æ¨¡å¼ ==========
                print(f"ğŸ¦ å¼€å§‹åˆ†æå›¾æ–‡ä¿¡æ¯")

                # è·å–å›¾æ–‡åˆ†æç»“æœ
                image_description = await self.llava_get_content(
                    prompt_setting.prue_image_analysis_template,
                    image_byte_content,
                    True,  # å›¾æ–‡æ··åˆ
                    user_question if user_question else ""  # ä¼ é€’ç”¨æˆ·æé—®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                )

                # æå–OCRæ–‡æœ¬
                ocr_text = self.target_file[0].page_content if self.target_file else ""
                print(f"ğŸŒ› OCRæ–‡æœ¬é•¿åº¦: {len(ocr_text)}")

                # æ„å»ºåŸºç¡€systemæ¶ˆæ¯
                system_message_parts = []
                if image_description:
                    system_message_parts.append(f"ã€å›¾ç‰‡åˆ†æç»“æœã€‘\n{image_description}")
                if ocr_text:
                    system_message_parts.append(f"ã€OCRæ–‡æœ¬å†…å®¹ã€‘\n{ocr_text}")

                # å¦‚æœæœ‰ç”¨æˆ·æé—®ï¼Œå°è¯•æ£€ç´¢çŸ¥è¯†åº“
                if user_question and user_question.strip():
                    print(f"ğŸ¯ æœ‰ç”¨æˆ·æé—®ï¼Œè¿›è¡Œæ„å›¾åˆ†æå’ŒçŸ¥è¯†åº“æŸ¥è¯¢")

                    intent_analysis_prompt = prompt_setting.image_intent_prompt.format(
                        image_description=image_description,
                        ocr_text=ocr_text
                    )
                    doc_types = self.analyze_intent_with_llm(intent_analysis_prompt)
                    print(f"ğŸˆ¶ æ„å›¾åˆ†æç»“æœ: {doc_types}")

                    if doc_types and len(doc_types) > 0:
                        relevant_docs = self.vector.query_by_question_vector_with_filter(
                            question_vector=user_question,
                            doc_types=doc_types,
                            top_k=3  # å‡å°‘æ•°é‡ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
                        )

                        if relevant_docs and len(relevant_docs) > 0:
                            # æ„å»ºçŸ¥è¯†åº“ä¸Šä¸‹æ–‡
                            knowledge_context = build_simple_context(relevant_docs)
                            system_message_parts.append(f"ã€ç›¸å…³çŸ¥è¯†åº“ä¿¡æ¯ã€‘\n{knowledge_context}")
                            print(f"ğŸ¯ çŸ¥è¯†åº“æ£€ç´¢åˆ° {len(relevant_docs)} æ¡ç›¸å…³ä¿¡æ¯")

                # å¦‚æœæ²¡æœ‰ç”¨æˆ·æé—®ï¼Œç›´æ¥è¿”å›åˆ†æç»“æœ
                if not user_question or user_question.strip() == "":
                    print("ğŸ¯ æ²¡æœ‰ç”¨æˆ·é—®é¢˜ï¼Œç›´æ¥è¿”å›å›¾æ–‡åˆ†æç»“æœ")
                    combined_content = "\n\n".join(system_message_parts)

                    # å°†ç»“æœæµå¼è¿”å›
                    chunk_size = 50
                    for i in range(0, len(combined_content), chunk_size):
                        chunk = combined_content[i:i + chunk_size]
                        data = {"choices": [{"delta": {"content": chunk}}]}
                        yield f"data: {json.dumps(data)}\n\n"
                        await asyncio.sleep(0.01)

                    yield "data: [DONE]\n\n"
                    return

                # æœ‰ç”¨æˆ·æé—®ï¼Œä½¿ç”¨å®Œæ•´çš„messageæ•°ç»„æ¨¡å¼
                system_message = "\n\n".join(system_message_parts)
                system_message += "\n\nè¯·æ ¹æ®å›¾ç‰‡å†…å®¹ã€OCRæ–‡æœ¬å’Œç›¸å…³çŸ¥è¯†åº“ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"

                # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯æ•°ç»„
                llm_messages = [{"role": "system", "content": system_message}]

                # æ·»åŠ å†å²æ¶ˆæ¯
                if self.messages:
                    for msg in self.messages:
                        normalized_msg = {"role": msg.get("role", "user"), "content": msg.get("content", "")}
                        if normalized_msg["content"].strip():
                            llm_messages.append(normalized_msg)

                print(f"ğŸ”„ å›¾æ–‡messageæ¨¡å¼: systemæ¶ˆæ¯é•¿åº¦ {len(system_message)}, æ¶ˆæ¯æ€»æ•° {len(llm_messages)}")

                # è°ƒç”¨æµå¼LLM
                async for chunk in stream_llm_response(llm_messages):
                    yield chunk

                yield "data: [DONE]\n\n"

        except Exception as e:
            print(f"âŒ å›¾ç‰‡åˆ†æå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            error_data = json.dumps({"error": str(e)})
            yield f"data: {error_data}\n\n"
            yield "data: [DONE]\n\n"


    def store_document_to_vector(self, chunks, doc_type):
        try:
            print(f"ğŸš€ å…±æœ‰{len(chunks)} è¿›è¡Œä¿å­˜ï¼Œæ–‡æ¡£ç±»å‹: {doc_type}")
            for i, chunk in enumerate(chunks):
                if hasattr(chunk, 'metadata'):
                    chunk.metadata['doc_type'] = doc_type
                else:
                    chunk.metadata = {'doc_type': doc_type}
            ids = self.vector.add_document_to_vector(chunks)
            print(f" stored {self.file_name} documents successfully")
            return ids
        except Exception as e:
                print(f" stored {self.file_name} documents failed: {str(e)}")
                raise e

    def del_knowledge_item(self, ids):
        corpus_ids = self.collation_ids(ids)

        result = []
        try:
            for corpus_id in corpus_ids:
                res = self.vector.delete_document(corpus_id)
                result.append(res)

        except Exception as e:
            print(f"åˆ é™¤å¤±è´¥å‘é‡æ•°æ®åº“æ•°æ®: {str(e)}")
            raise e

        if None in result:
            return None
        else:
            return True

    def clear_all_documents(self):
        self.vector.clear_collection()

    def _should_use_historical_intent(self):
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å†å²æ„å›¾
        åŸºäºç®€å•è§„åˆ™ï¼šé—®é¢˜æ¨¡ç³Šã€ç®€çŸ­ã€ä¸”å†å²æ„å›¾å­˜åœ¨
        """
        # è§„åˆ™1ï¼šæœ‰å¯ç”¨çš„å†å²æ„å›¾
        if not self.last_doc_types:
            return False

        # è§„åˆ™2ï¼šå½“å‰é—®é¢˜ç®€çŸ­æˆ–æ¨¡ç³Š
        question = self.question.strip()
        if len(question) >= 10:
            # é—®é¢˜è¶³å¤Ÿæ˜ç¡®ï¼Œåº”è¯¥ç‹¬ç«‹åˆ†æ
            return False

        # è§„åˆ™3ï¼šåŒ…å«å»¶ç»­æ€§å…³é”®è¯
        continuation_keywords = ["æ›´å¤š", "è¯¦ç»†", "è¿˜æœ‰", "æ¥ç€", "ç»§ç»­", "More", "Details", "Also", "Next", "Continue"]
        if any(keyword in question for keyword in continuation_keywords):
            return True

        # è§„åˆ™4ï¼šé—®é¢˜å¾ˆçŸ­ï¼ˆå¯èƒ½æ˜¯å›åº”å¼æé—®ï¼‰
        if len(question) <= 8:
            return True

        # è§„åˆ™5ï¼šæ£€æŸ¥å¯¹è¯å†å²è¿ç»­æ€§
        if self.messages and len(self.messages) >= 2:
            # è·å–æœ€è¿‘ä¸€æ¬¡åŠ©æ‰‹å›ç­”
            last_assistant_msg = None
            for msg in reversed(self.messages[:-1]):  # æ’é™¤å½“å‰æ¶ˆæ¯
                if msg.get("role") == "assistant":
                    last_assistant_msg = msg.get("content", "")
                    break

        return False


    def question_query_from_vector(self):
        """
        æ–°é€»è¾‘ï¼šä½¿ç”¨LLMåˆ†ææ„å›¾ï¼Œç„¶åè¿›è¡Œè¿‡æ»¤æŸ¥è¯¢
        """
        print(f"ğŸ” æ‰§è¡Œå‘é‡æŸ¥è¯¢ï¼Œé—®é¢˜: '{self.question}'")

        # 1. ä½¿ç”¨LLMåˆ†ææ„å›¾
        intent_prompt = prompt_setting.intent_analysis_template.replace('{question}', self.question)
        current_doc_types = self.analyze_intent_with_llm(intent_prompt)

        # 2. å¦‚æœå½“å‰æ„å›¾ä¸ºç©ºï¼Œåˆ¤æ–­æ˜¯å¦ä½¿ç”¨å†å²æ„å›¾
        if not current_doc_types and self._should_use_historical_intent():
            print(f"ğŸ¯ ä½¿ç”¨å†å²æ„å›¾: {self.last_doc_types}")
            doc_types = self.last_doc_types
        else:
            doc_types = current_doc_types
            if doc_types:
                self.last_doc_types = doc_types
                print(f"ğŸ“ æ›´æ–°å†å²æ„å›¾ä¸º: {doc_types}")
            elif self._should_use_historical_intent():
                print(f"ğŸ“ ä½¿ç”¨å†å²æ„å›¾ä½†ä¸æ›´æ–°ï¼ˆå› ä¸ºå½“å‰æ„å›¾ä¸æ˜ç¡®: {doc_types}")
                doc_types = self.last_doc_types
            else:
                print(f"ğŸ“ æ„å›¾ä¸æ˜ç¡®ï¼Œæ¸…ç©ºå†å²æ„å›¾ï¼ˆè¯é¢˜å¯èƒ½å·²ç»“æŸï¼‰: {doc_types}")
                self.last_doc_types = []
                doc_types = []

        # 3. å¦‚æœæœ‰åŒ¹é…çš„doc_typeï¼Œè¿›è¡Œè¿‡æ»¤æŸ¥è¯¢
        if doc_types and len(doc_types) > 0:
            print(f"ğŸ¯ ä½¿ç”¨è¿‡æ»¤æŸ¥è¯¢ (ç›®æ ‡åˆ†åŒº: {doc_types})")

            # ä½¿ç”¨è¿‡æ»¤æŸ¥è¯¢
            results = self.vector.query_by_question_vector_with_filter(
                question_vector=self.question,
                doc_types=doc_types,
                top_k=5  # åªéœ€è¦5ä¸ªæœ€ä¼˜ç»“æœ

            )

            if results and len(results) > 0:
                return results
            else:
                print(f"âš ï¸ è¿‡æ»¤æŸ¥è¯¢æ— ç»“æœï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ç±»å‹çš„å†…å®¹")
                return []
        else:
            print(f"ğŸ¯ æ— åŒ¹é…çš„æ–‡æ¡£ç±»å‹ï¼ŒçŸ¥è¯†åº“æ²¡æœ‰ç›¸å…³ä¿¡æ¯")
            return []

    def get_chunk_doc(self, target_file, clear_chunks=False):
        try:
            print(f"ğŸš€ start split {self.file_name}")
            splitter_chunks = TextSplitter().split_document(target_file)

            if clear_chunks:
                chunks = self.clear_data(splitter_chunks)
            else:
                chunks = splitter_chunks
            return chunks
        except Exception as e:
            print(f" split error: {e}")
            raise e

    async def stream_context_from_docs(self, documents):
        """æµå¼ç”Ÿæˆä¸Šä¸‹æ–‡ - æ­£ç¡®çš„å¤šè½®å¯¹è¯å¤„ç†"""
        # æ„å»ºæ¶ˆæ¯æ•°ç»„
        llm_messages = []

        # 1. å¦‚æœæœ‰çŸ¥è¯†åº“ä¿¡æ¯ï¼Œä½œä¸ºsystemæ¶ˆæ¯
        if documents:
            context_str = build_simple_context(documents)
            system_content =  prompt_setting.knowledge_history_template.format(context_str=context_str)
            llm_messages.append({
                "role": "system",
                "content": system_content
            })

        # 2. æ£€æŸ¥æ˜¯å¦æ˜¯æŒ‡ä»£å›¾ç‰‡çš„é—®é¢˜
        is_image_reference = False
        image_reference_text = ""

        if self.messages and len(self.messages) >= 2:
            current_question = self.question or ""
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡ç›¸å…³çš„æŒ‡ä»£è¯
            image_keywords = ["å›¾", "å›¾ç‰‡", "ç…§ç‰‡", "æˆªå›¾", "ç”»é¢", "å›¾åƒ", "photo", "image"]
            has_image_keyword = any(keyword in current_question for keyword in image_keywords)

            if has_image_keyword:
                # æŸ¥æ‰¾æœ€è¿‘çš„å›¾ç‰‡æè¿°
                for i in range(len(self.messages) - 2, -1, -1):  # ä»å€’æ•°ç¬¬äºŒæ¡å¾€å‰æ‰¾
                    msg = self.messages[i]
                    if isinstance(msg, dict):
                        # æ£€æŸ¥æ˜¯å¦æ˜¯assistantçš„å›å¤ä¸”åŒ…å«å›¾ç‰‡æè¿°ç‰¹å¾
                        content = msg.get("content", "")
                        if ("è¿™æ˜¯ä¸€å¼ " in content or "ç…§ç‰‡" in content or
                                "åœºæ™¯" in content or "ç”»é¢" in content):
                            is_image_reference = True
                            image_reference_text = content
                            break

        # 2. ç›´æ¥ä¼ é€’åŸå§‹å¯¹è¯å†å²ï¼ˆå‰ç«¯å·²é™åˆ¶æ•°é‡ï¼‰
        if self.messages:
            # ç¡®ä¿æ ¼å¼æ­£ç¡®
            for msg in self.messages:
                normalized_msg = {}
                # è½¬æ¢role
                if "type" in msg:
                    normalized_msg["role"] = "user" if msg["type"] == "user" else "assistant"
                elif "role" in msg:
                    normalized_msg["role"] = msg["role"]
                else:
                    normalized_msg["role"] = "user"  # é»˜è®¤

                # ç¡®ä¿contentå­˜åœ¨
                if "content" in msg:
                    normalized_msg["content"] = msg["content"]
                elif "text" in msg:
                    normalized_msg["content"] = msg["text"]
                else:
                    normalized_msg["content"] = ""

                # åªæ·»åŠ æœ‰å†…å®¹çš„message
                if normalized_msg["content"].strip():
                    llm_messages.append(normalized_msg)

        if is_image_reference and image_reference_text:
            # åœ¨æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯åæ·»åŠ ç³»ç»Ÿæç¤º
            for i in range(len(llm_messages) - 1, -1, -1):
                if llm_messages[i].get("role") == "user":
                    # ä¿®æ”¹å½“å‰ç”¨æˆ·é—®é¢˜ï¼Œæ˜ç¡®å¼•ç”¨å›¾ç‰‡æè¿°
                    original_content = llm_messages[i]["content"]
                    enhanced_content = f"""{original_content}

                ï¼ˆæç¤ºï¼šæ ¹æ®ä¹‹å‰çš„å¯¹è¯ï¼Œå›¾ç‰‡æè¿°ä¸ºï¼š{image_reference_text[:200]}...è¯·åŸºäºè¿™ä¸ªå›¾ç‰‡æè¿°å›ç­”ã€‚ï¼‰"""
                    llm_messages[i]["content"] = enhanced_content
                    break


        print(f"ğŸ”„ æ–‡æœ¬æ¨¡å¼:å¼€å§‹æµå¼ç”Ÿæˆï¼Œæ¶ˆæ¯æ€»æ•°: {len(llm_messages)}")

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        try:
            # è°ƒç”¨æµå¼LLMï¼Œä¼ é€’æ­£ç¡®çš„messagesæ•°ç»„
            async for chunk in stream_llm_response(llm_messages):
                if chunk:
                    yield chunk

            yield "data: [DONE]\n\n"
            end_time = time.time()
            print(f"âœ… æµå¼ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")

        except Exception as e:
            print(f"âŒ æµå¼ç”Ÿæˆå¼‚å¸¸: {e}")
            error_data = json.dumps({"error": str(e)})
            yield f"data: {error_data}\n\n"
            yield "data: [DONE]\n\n"


    async def upload_infor_to_vector(self):
        try:
            if self.file_type != 'image':
                print(f" âœ… å¼€å§‹è¿›è¡Œä¿å­˜çŸ¥è¯†åº“æ“ä½œ, ä¸Šä¼ çš„çŸ¥è¯†ç±»å‹{self.doc_type}")
                chunks = self.get_chunk_doc(self.target_file)
                stored_ids = self.store_document_to_vector(chunks, self.doc_type)
                return stored_ids
            else:
                print(f"ä¸èƒ½ä¸Šä¼ å›¾ç‰‡")
                pass
        except Exception as e:
            print(f"âŒå­˜å‚¨å‘é‡æ•°æ®åº“å¤±è´¥ {str(e)}")

    def clear_data(self, chunks):
        all_rag_chunks = []

        for j, chunk in enumerate(chunks):
            cur_document = Document(
                page_content=chunk.page_content,
                metadata={
                    "source":self.file_name,
                    "chunk":j
                }
            )
            all_rag_chunks.append(cur_document)
        clearner = AdvancedTextCleaner()
        cleaned_chunks = clearner.clean_documents(all_rag_chunks)
        print(f" from {len(all_rag_chunks)} remove {len(cleaned_chunks)} ")
        return cleaned_chunks

    def collation_ids(self, ids):
        data_dict = dict(ids)
        corpus_ids = data_dict.get("corpus_ids", [])
        return corpus_ids


    def dev_env_test_api(self):
        self.vector.verify_doc_type_storage()
        # éªŒè¯ç‰¹å®šç±»å‹
        # self.vector.verify_doc_type_storage("resume")
        # self.vector.verify_doc_type_storage("code")

    def analyze_intent_with_llm(self, question):
        """
        ä½¿ç”¨LLMåˆ†æé—®é¢˜æ„å›¾ï¼Œè¿”å›å¯èƒ½çš„doc_typeæ•°ç»„
        """
        try:
            result = connect_text_llm(question)

            # ç®€åŒ–å¤„ç†ï¼šç›´æ¥æå–content
            if isinstance(result, dict):
                content = result.get('content', '')
            else:
                content = str(result)

            # å°è¯•è§£æJSON
            import json
            import re

            # æ¸…ç†content
            content = content.strip()

            # æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                try:
                    json_str = json_match.group()
                    content_dict = json.loads(json_str)
                    doc_types = content_dict.get('doc_types', [])
                    print(f"ğŸ¯ LLMæ„å›¾åˆ†æç»“æœ: {doc_types}")
                    return doc_types
                except json.JSONDecodeError:
                    print(f"âŒ JSONè§£æå¤±è´¥ï¼Œå†…å®¹: {content[:100]}...")

            print(f"âš ï¸ æœªèƒ½è§£ædoc_typesï¼Œè¿”å›ç©ºæ•°ç»„")
            return []

        except Exception as e:
            print(f"âŒ LLMæ„å›¾åˆ†æå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return []

    # async def run_by_web(self):
    #     print(f"ğŸš€ Rag started at {datetime.datetime.now()} ")
    #     try:
    #         loader = DocumentLoader(urls=["https://tailwindcss.com/docs/installation/using-vite"])
    #         docs = await loader.load()
    #         text = "\n".join([doc.page_content for doc in docs])
    #         chunks = TextSplitter().split_text(text)
    #         print(f" åˆ†å‰²å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
    #         return chunks
    #     except Exception as e:
    #         print(f" æ‰§è¡Œå¤±è´¥: {e}")
    #         raise

    # async def main():
    #     app = RagService()
    #     res = await app.run_rag()
    #     for i, chunk in enumerate(res):
    #         print(f"--- åˆ†å— {i + 1} ---\n{chunk[:200]}...")
    #
    # asyncio.run(main())

