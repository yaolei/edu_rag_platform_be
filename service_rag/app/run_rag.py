from typing import Optional, List
from fastapi import UploadFile
from service_rag.app.embedding.embedding_data import EmbeddingData
from service_rag.app.prompt.prompt import prompt_setting
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document
from service_rag.app.document_operation.document_loader import DocumentLoader
from service_rag.app.text_splitter.text_split import TextSplitter
from service_rag.app.vector.vector_store import VectorStore
from service_rag.app.llm_model.contect_llm import  connect_text_llm, analyze_with_image
from service_rag.app.text_splitter.advanced_text_cleaner import AdvancedTextCleaner
from pathlib import Path
import base64

class RagService:
    def __init__(self):
        self.prompt = PromptTemplate(input_variables=['context', 'question'],
                                     template=prompt_setting.rag_template)
        self.embedding_type = None
        self.upload_file = None
        self.file_name = None
        self.file_name_without_extension = None
        self.target_file = None
        self.embeddings = None
        self.vector = None
        self.question = None
        self.file_type = None
        self.if_files = None
        self.doc_type = None
        self.mutil_files = []

    @classmethod
    async def create(cls, upload_file: List[UploadFile]=None, embedding_type='questions', doc_type="document",
                     question:Optional[str] = None, **kwargs):
        instance = cls()
        await instance.initialize(upload_file, embedding_type, doc_type, question, **kwargs)
        return instance

    async def initialize(self, upload_file: List[UploadFile]=None, embedding_type='questions', doc_type="document",
                         question:Optional[str] = None, **kwargs):
        self.embedding_type = embedding_type
        self.question = question
        self.upload_file = upload_file
        self.doc_type = doc_type
        self.embeddings = EmbeddingData(embedding_type=embedding_type)
        self.vector = VectorStore(embedding_function=self.embeddings)
        if not upload_file:  # æ— æ–‡ä»¶
            pass
        elif len(upload_file) == 1:
            self.if_files = False
            self.file_name = upload_file[0].filename or "unknown file"
            path_obj = Path(self.file_name)
            self.file_name_without_extension = path_obj.stem

            try:
                document_loader = DocumentLoader(upload_file[0])
                self.target_file = await document_loader.load()
                document_loader.cleanup_temp_resources()
                print(f"ğŸš€ğŸš€ğŸš€ğŸš€{ self.target_file} ğŸš€ğŸš€")
                if self.target_file and self.target_file[0].page_content == '':
                    self.target_file = None
                else:
                    if 'document_loader' in locals():
                        document_loader.cleanup_temp_resources()
                    self.file_type = document_loader._detect_document_type()
                    return False
            except Exception as e:
                print(f"âŒ embedding module error: {str(e)}")
                raise e
        else:
            self.if_files = True
            for f in upload_file:
                document_loader_muti_file = DocumentLoader(f)
                self.mutil_files.append(await document_loader_muti_file.load())
                document_loader_muti_file.cleanup_temp_resources()
            print(f"ğŸ¯ {self.mutil_files} ğŸ¯")

    async def analyse_image_information(self):
        """
        é‡æ„åçš„å›¾ç‰‡åˆ†ææµç¨‹ï¼š
        1. å…ˆç”¨æç®€æŒ‡ä»¤è®©LLaVAåˆ†æå›¾ç‰‡ï¼Œå¾—åˆ°å®¢è§‚æè¿°ã€‚
        2. ç”¨æè¿°ä¸­çš„å…³é”®è¯å»æŸ¥è¯¢å‘é‡æ•°æ®åº“ã€‚
        3. æœ€åç»“åˆæè¿°å’ŒçŸ¥è¯†åº“ï¼Œç”¨æ–‡æœ¬æ¨¡å‹ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚
        """
        print(f"ğŸ¦ å¼€å§‹åˆ†æå›¾åƒä¿¡æ¯ï¼Œé—®é¢˜: {self.question} ğŸ¦")

        # 0. ç›´æ¥è¯»å–å›¾ç‰‡æ–‡ä»¶ (æ— è®ºæœ‰æ— OCRæ–‡æœ¬ï¼Œéƒ½éœ€è¦åˆ†æå›¾ç‰‡)
        upload_file = self.upload_file[0]
        content = await upload_file.read()
        base64_str = base64.b64encode(content).decode("utf-8")
        image_data_url = f"data:{upload_file.content_type};base64,{base64_str}"
        print(f"ğŸ¦ å¤„ç†æ–‡ä»¶: {upload_file.filename}")

        # ========== ç¬¬ä¸€æ­¥ï¼šè®©å›¾ç‰‡æ¨¡å‹è¿›è¡ŒåŸºç¡€åˆ†æ ==========
        print(f"ğŸ¦ æ­¥éª¤1: è°ƒç”¨LLaVAè¿›è¡ŒåŸºç¡€å›¾ç‰‡åˆ†æ...")
        # ä½¿ç”¨ä¸€ä¸ªæç®€ã€èšç„¦çš„æç¤ºè¯ï¼Œåªè¦æ±‚æè¿°
        image_analysis_prompt = "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„åœºæ™¯ã€ä¸»è¦å†…å®¹ã€ç‰©ä½“ã€é¢œè‰²å’Œæ°›å›´ã€‚"
        image_analysis_result = analyze_with_image(
            image_base64_data_url=image_data_url,
            question=image_analysis_prompt  # ä¼ å…¥ç®€çŸ­çš„ã€åªå…³äºå›¾ç‰‡æœ¬èº«çš„é—®é¢˜
        )

        # æå–å›¾ç‰‡æè¿°æ–‡æœ¬
        if isinstance(image_analysis_result, dict) and 'content' in image_analysis_result:
            image_description = image_analysis_result['content'].strip()
        else:
            image_description = str(image_analysis_result).strip()

        # å¤„ç†LLaVAè¾“å‡ºä¹±ç çš„æƒ…å†µï¼šå¦‚æœæè¿°å¼‚å¸¸ç®€çŸ­æˆ–åŒ…å«å¤§é‡é‡å¤å­—ç¬¦ï¼Œè§†ä¸ºå¤±è´¥
        if len(image_description) < 50 or "å¹…å¹…å¹…" in image_description:
            print(f"âŒ LLaVAåˆ†æå¤±è´¥æˆ–è¾“å‡ºå¼‚å¸¸ï¼Œç›´æ¥ä½¿ç”¨å¤‡ç”¨æç¤ºã€‚")
            image_description = f"ç”¨æˆ·ä¸Šä¼ äº†ä¸€å¼ å›¾ç‰‡ï¼Œæ–‡ä»¶åä¸ºï¼š{upload_file.filename}ã€‚"

        print(f"ğŸ¦ è·å¾—çš„å›¾ç‰‡æè¿°æ‘˜è¦: {image_description[:150]}...")

        # ========== ç¬¬äºŒæ­¥ï¼šåŸºäºå›¾ç‰‡æè¿°æŸ¥è¯¢çŸ¥è¯†åº“ ==========
        print(f"ğŸ¦ æ­¥éª¤2: åŸºäºå›¾ç‰‡æè¿°æŸ¥è¯¢çŸ¥è¯†åº“...")
        # ä½¿ç”¨å›¾ç‰‡æè¿°ï¼ˆè€Œä¸æ˜¯OCRæ–‡æœ¬ï¼‰ä½œä¸ºæŸ¥è¯¢ä¾æ®
        query_for_vector = f"æ ¹æ®ä»¥ä¸‹å›¾ç‰‡æè¿°ï¼ŒæŸ¥æ‰¾ç›¸å…³çŸ¥è¯†ï¼š{image_description[:500]}"  # é™åˆ¶é•¿åº¦
        relevant_docs = self.vector.query_by_question_vector(query_for_vector)

        knowledge_context = ""
        if relevant_docs and relevant_docs != "False" and len(str(relevant_docs).strip()) > 10:
                try:
                    if isinstance(relevant_docs, list):
                        # æå–æ¯ä¸ªdocçš„textå­—æ®µ
                        text_list = []
                        for doc in relevant_docs:
                            if isinstance(doc, dict) and 'text' in doc:
                                text_list.append(doc['text'])
                            elif hasattr(doc, 'page_content'):  # å¦‚æœæ˜¯Documentå¯¹è±¡
                                text_list.append(doc.page_content)
                        knowledge_context = "\n\n".join(text_list)
                    else:
                        knowledge_context = str(relevant_docs)
                    print(f"ğŸ¦ æ‰¾åˆ°ç›¸å…³çŸ¥è¯†ç‚¹ï¼Œé•¿åº¦: {len(knowledge_context)}")
                except Exception as e:
                    print(f"âŒ æå–textå­—æ®µæ—¶å‡ºé”™: {str(e)}")
        else:
            knowledge_context = "çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ä¸å›¾ç‰‡ç›´æ¥ç›¸å…³çš„ä¿¡æ¯ã€‚"
            print(f"ğŸ¦ æœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯")

        # ========== ç¬¬ä¸‰æ­¥ï¼šç»¼åˆä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆå›ç­” ==========
        print(f"ğŸ¦ æ­¥éª¤3: ç»¼åˆå›¾ç‰‡æè¿°ä¸çŸ¥è¯†åº“ä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”...")
        # æ„å»ºç»™æ–‡æœ¬æ¨¡å‹çš„æç¤ºè¯
        final_prompt_for_text_model = f"""
                è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
            
                ã€å›¾ç‰‡åˆ†æç»“æœã€‘
                {image_description}
            
                ã€ç›¸å…³èƒŒæ™¯çŸ¥è¯†ã€‘
                {knowledge_context}
            
                ã€ç”¨æˆ·æå‡ºçš„é—®é¢˜ã€‘
                {self.question if self.question else 'è¯·åˆ†æè¿™å¼ å›¾ç‰‡ã€‚'}
            
                è¯·å°†å›¾ç‰‡åˆ†æç»“æœå’Œç›¸å…³èƒŒæ™¯çŸ¥è¯†æœ‰æœºç»“åˆï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´ã€æµç•…çš„å›ç­”ã€‚å¦‚æœèƒŒæ™¯çŸ¥è¯†æ˜¾ç¤ºâ€œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯â€ï¼Œåˆ™ä¸»è¦ä¾æ®å›¾ç‰‡åˆ†æç»“æœå›ç­”ã€‚
                å›ç­”çš„å¼€å¤´è¯·åŠ ä¸Šï¼šâ€œEvan è®©æ‚¨ä¹…ç­‰äº†ã€‚â€
                """
        # è°ƒç”¨ä½ çš„æ–‡æœ¬èŠå¤©å‡½æ•°
        final_answer = connect_text_llm(
            question=final_prompt_for_text_model  # è¿™é‡Œä¼ å…¥æ•´åˆäº†æ‰€æœ‰ä¿¡æ¯çš„æç¤º
        )

        # å¤„ç†æœ€ç»ˆç»“æœ
        if isinstance(final_answer, dict) and 'content' in final_answer:
            result_content = final_answer['content']
        else:
            result_content = str(final_answer)

        print(f"ğŸ¦ æœ€ç»ˆå›ç­”ç”Ÿæˆå®Œæ¯•ï¼Œé•¿åº¦: {len(result_content)}")
        return result_content
    def store_document_to_vector(self, chunks, doc_type):
        try:
            print(f"ğŸš€ å…±æœ‰{len(chunks)} è¿›è¡Œä¿å­˜ï¼Œæ–‡æ¡£ç±»å‹: {doc_type}")
            for i, chunk in enumerate(chunks):
                if hasattr(chunk, 'metadata'):
                    chunk.metadata['doc_type'] = doc_type
                else:
                    chunk.metadata = {'doc_type': doc_type}
            ids = self.vector.add_document_to_vector(chunks)
            print(f" stored {self.file_name_without_extension} documents successfully")
            return ids
        except Exception as e:
                print(f" stored {self.file_name_without_extension} documents failed: {str(e)}")
                raise e

    def del_knowledge_item(self, ids):
        corpus_ids = self.collation_ids(ids)

        result = []
        try:
            for corpus_id in corpus_ids:
                res = self.vector.delete_document(corpus_id)
                result.append(res)

        except Exception as e:
            print(f"åˆ é™¤å¤±è´¥å‘é‡æ•°æ®åº“æ•°æ®: {str(e)}")
            raise e

        if None in result:
            return None
        else:
            return True

    def clear_all_documents(self):
        self.vector.clear_collection()

    def question_query_from_vector(self):
        """
        æ–°é€»è¾‘ï¼šä½¿ç”¨LLMåˆ†ææ„å›¾ï¼Œç„¶åè¿›è¡Œè¿‡æ»¤æŸ¥è¯¢
        """
        print(f"ğŸ” æ‰§è¡Œå‘é‡æŸ¥è¯¢ï¼Œé—®é¢˜: '{self.question}'")

        # 1. ä½¿ç”¨LLMåˆ†ææ„å›¾
        doc_types = self.analyze_intent_with_llm(self.question)

        # 2. å¦‚æœæœ‰åŒ¹é…çš„doc_typeï¼Œè¿›è¡Œè¿‡æ»¤æŸ¥è¯¢
        if doc_types and len(doc_types) > 0:
            print(f"ğŸ¯ ä½¿ç”¨è¿‡æ»¤æŸ¥è¯¢ (ç›®æ ‡åˆ†åŒº: {doc_types})")

            # ä½¿ç”¨è¿‡æ»¤æŸ¥è¯¢
            results = self.vector.query_by_question_vector_with_filter(
                question_vector=self.question,
                doc_types=doc_types,
                top_k=5  # åªéœ€è¦5ä¸ªæœ€ä¼˜ç»“æœ
            )

            if results and len(results) > 0:
                print(f"âœ… è¿‡æ»¤æŸ¥è¯¢å®Œæˆ: {len(results)} ä¸ªç»“æœ")
                return results
            else:
                print(f"âš ï¸ è¿‡æ»¤æŸ¥è¯¢æ— ç»“æœï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ç±»å‹çš„å†…å®¹")
                return []
        else:
            # 3. å¦‚æœæ²¡æœ‰åŒ¹é…çš„doc_typeï¼ŒçŸ¥è¯†åº“æ²¡æœ‰ç›¸å…³ä¿¡æ¯
            print(f"ğŸ¯ æ— åŒ¹é…çš„æ–‡æ¡£ç±»å‹ï¼ŒçŸ¥è¯†åº“æ²¡æœ‰ç›¸å…³ä¿¡æ¯")
            return []

    def get_chunk_doc(self, target_file, clear_chunks=False):
        try:
            print(f"ğŸš€ start split {self.file_name_without_extension}")
            splitter_chunks = TextSplitter().split_document(target_file)

            if clear_chunks:
                chunks = self.clear_data(splitter_chunks)
                print(f"ğŸš€ ğŸš€ ğŸš€  {chunks}")
            else:
                chunks = splitter_chunks
            return chunks
        except Exception as e:
            print(f" split error: {e}")
            raise e

    def get_context_from_docs(self, documents):
        """æ„å»ºä¸Šä¸‹æ–‡"""
        if not documents:
            formatter_prompt = prompt_setting.no_knowledge_template.replace(
                '{question}', self.question
            )
        else:
            context_str = self._build_simple_context(documents)
            formatter_prompt = prompt_setting.rag_template_pro.replace(
                '{context}', context_str
            ).replace(
                '{question}', self.question
            )

        print(f"âœ… æœ€ç»ˆPrompté•¿åº¦: {len(formatter_prompt)} å­—ç¬¦")
        return connect_text_llm(formatter_prompt)

    def _build_simple_context(self, documents):
        """æ„å»ºçº¯å‡€çš„ä¸Šä¸‹æ–‡ï¼Œå»æ‰å†…éƒ¨æ ‡è®°å’Œå…ƒæ•°æ®"""
        if not documents:
            return ""

        context_parts = []
        for i, doc in enumerate(documents[:5]):  # æœ€å¤š5ä¸ª
            content = ""

            if isinstance(doc, dict):
                content = doc.get('text', '')
                if not content:
                    content = doc.get('page_content', '')
                    if not content and hasattr(doc, 'get'):
                        # å°è¯•è·å–ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²å€¼
                        for key, value in doc.items():
                            if isinstance(value, str) and len(value.strip()) > 0:
                                content = value
                                break
            elif hasattr(doc, 'page_content'):
                # Documentå¯¹è±¡
                content = doc.page_content

            if content:
                content = content.strip()
                import re
                content = re.sub(r'\s+', ' ', content)

                # åªæ·»åŠ éç©ºå†…å®¹
                if content:
                    context_parts.append(content)

        if not context_parts:
            return ""

        return "\n\n---\n\n".join(context_parts)

    async def run_rag_engine(self):
        if self.embedding_type == 'questions':
            print(f"âœ…è¿›å…¥é—®ç­”åœºæ™¯....")
            res_doc = self.question_query_from_vector()
            try:
                print(f"ğŸš€ start query answer by LLM...")
                return self.get_context_from_docs(res_doc)
            except Exception as e:
                print(f"âŒğŸ”¥ {str(e)}")
                raise e
        else:
            if self.file_type !='image':
                print(f" âœ… å¼€å§‹è¿›è¡Œä¿å­˜çŸ¥è¯†åº“æ“ä½œ, ä¸Šä¼ çš„çŸ¥è¯†ç±»å‹{self.doc_type}")
                print(f" ä¸Šä¼ çš„æ–‡ä»¶åç§°: {self.file_name_without_extension}")
                chunks = self.get_chunk_doc(self.target_file)
                stored_ids = self.store_document_to_vector(chunks, self.doc_type)
                return stored_ids
            else:
                print(f"ä¸èƒ½ä¸Šä¼ å›¾ç‰‡")
                pass


    def clear_data(self, chunks):
        all_rag_chunks = []

        for j, chunk in enumerate(chunks):
            cur_document = Document(
                page_content=chunk.page_content,
                metadata={
                    "source":self.file_name,
                    "chunk":j
                }
            )
            all_rag_chunks.append(cur_document)
        clearner = AdvancedTextCleaner()
        cleaned_chunks = clearner.clean_documents(all_rag_chunks)
        print(f" from {len(all_rag_chunks)} remove {len(cleaned_chunks)} ")
        return cleaned_chunks

    def collation_ids(self, ids):
        data_dict = dict(ids)
        corpus_ids = data_dict.get("corpus_ids", [])
        return corpus_ids


    def dev_env_test_api(self):
        self.vector.verify_doc_type_storage()
        # éªŒè¯ç‰¹å®šç±»å‹
        # self.vector.verify_doc_type_storage("resume")
        # self.vector.verify_doc_type_storage("code")

    def analyze_intent_with_llm(self, question):
        """
        ä½¿ç”¨LLMåˆ†æé—®é¢˜æ„å›¾ï¼Œè¿”å›å¯èƒ½çš„doc_typeæ•°ç»„
        """
        try:
            # ä½¿ç”¨prompt.pyä¸­çš„æ„å›¾åˆ†ææ¨¡æ¿
            intent_prompt = prompt_setting.intent_analysis_template.replace('{question}', question)

            print(f"ğŸ¯ å‘é€ç»™LLMçš„æ„å›¾åˆ†æè¯·æ±‚: {intent_prompt[:200]}...")

            # ç›´æ¥ä¼ é€’å­—ç¬¦ä¸²å‚æ•°
            result = connect_text_llm(intent_prompt)

            # è°ƒè¯•ï¼šæ‰“å°resultçš„ç±»å‹å’Œå†…å®¹
            print(f"ğŸ¯ connect_text_llmè¿”å›ç±»å‹: {type(result)}")
            print(f"ğŸ¯ connect_text_llmè¿”å›å€¼: {result}")

            # å¤„ç†è¿”å›ç»“æœ
            content_dict = {}
            if isinstance(result, dict):
                print(f"ğŸ¯ resultæ˜¯å­—å…¸ï¼Œkeys: {result.keys()}")
                content = result.get('content', '')

                # é‡è¦ï¼šcontentå¯èƒ½æ˜¯å­—å…¸ï¼Œä¹Ÿå¯èƒ½æ˜¯å­—ç¬¦ä¸²
                if isinstance(content, dict):
                    content_dict = content
                elif isinstance(content, str):
                    # å°è¯•è§£æå­—ç¬¦ä¸²ä¸ºå­—å…¸
                    import json
                    try:
                        content_dict = json.loads(content)
                    except json.JSONDecodeError:
                        # å¦‚æœä¸æ˜¯JSONï¼Œå°è¯•æå–JSON
                        import re
                        json_match = re.search(r'\{.*\}', content, re.DOTALL)
                        if json_match:
                            try:
                                content_dict = json.loads(json_match.group())
                            except:
                                pass

            # ä»content_dictä¸­æå–doc_types
            if isinstance(content_dict, dict) and 'doc_types' in content_dict:
                doc_types = content_dict['doc_types']
                print(f"ğŸ¯ LLMæ„å›¾åˆ†æç»“æœ: {doc_types}")
                return doc_types

            # å¦‚æœä»¥ä¸Šéƒ½å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…
            return self._fallback_intent_analysis(question)

        except Exception as e:
            print(f"âŒ LLMæ„å›¾åˆ†æå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            # è¿”å›é»˜è®¤å€¼
            return ['document']

    def _fallback_intent_analysis(self, question):
        """å¤‡ç”¨æ„å›¾åˆ†ææ–¹æ³•ï¼šåŸºäºå…³é”®è¯åŒ¹é…"""
        question_lower = question.lower()
        doc_types = []

        # ç®€å†ç›¸å…³å…³é”®è¯
        if any(word in question_lower for word in
               ['ç®€å†', 'æ±‚èŒ', 'å€™é€‰äºº', 'å¼€å‘è€…', 'ç»éªŒ', 'æ‹›è˜', 'æ¨è', 'å·¥ä½œç»å†', 'é¡¹ç›®ç»éªŒ']):
            doc_types.append('resume')
        # ä»£ç ç›¸å…³å…³é”®è¯
        if any(word in question_lower for word in ['ä»£ç ', 'ç¼–ç¨‹', 'æŠ€æœ¯æ ˆ', 'å¼€å‘', 'ç¨‹åº', 'bug']):
            doc_types.append('code')
        # å›¾ç‰‡ç›¸å…³å…³é”®è¯
        if any(word in question_lower for word in ['å›¾ç‰‡', 'å›¾åƒ', 'ç…§ç‰‡', 'å›¾']):
            doc_types.append('image_desc')
        # æ–‡æ¡£ç›¸å…³å…³é”®è¯
        if any(word in question_lower for word in ['æ–‡æ¡£', 'æ–‡ä»¶', 'èµ„æ–™']):
            doc_types.append('document')

        if not doc_types:
            doc_types.append('document')  # é»˜è®¤

        print(f"ğŸ¯ å…³é”®è¯åŒ¹é…æ„å›¾åˆ†æç»“æœ: {doc_types}")
        return doc_types

    # async def run_by_web(self):
    #     print(f"ğŸš€ Rag started at {datetime.datetime.now()} ")
    #     try:
    #         loader = DocumentLoader(urls=["https://tailwindcss.com/docs/installation/using-vite"])
    #         docs = await loader.load()
    #         text = "\n".join([doc.page_content for doc in docs])
    #         chunks = TextSplitter().split_text(text)
    #         print(f" åˆ†å‰²å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
    #         return chunks
    #     except Exception as e:
    #         print(f" æ‰§è¡Œå¤±è´¥: {e}")
    #         raise

    # async def main():
    #     app = RagService()
    #     res = await app.run_rag()
    #     for i, chunk in enumerate(res):
    #         print(f"--- åˆ†å— {i + 1} ---\n{chunk[:200]}...")
    #
    # asyncio.run(main())

